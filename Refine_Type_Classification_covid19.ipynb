{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sp\n",
    "from scipy.integrate import odeint\n",
    "import networkx as nx\n",
    "from pybel.examples import sialic_acid_graph as sag\n",
    "import pybel as pb\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pybel.io.jupyter import to_jupyter\n",
    "import torch\n",
    "import pyro\n",
    "import covid19kg\n",
    "import pandas as pd\n",
    "\n",
    "pyro.set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create generic discrete probability function\n",
    "class cg_node():\n",
    "    def __init__(self,n_inputs,name):\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.name = name\n",
    "        \n",
    "        if n_inputs == 0:\n",
    "            self.label = 'exogenous'\n",
    "        else:\n",
    "            self.label = 'endogenous'\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def p_init(self,input_data,var_data):\n",
    "        \n",
    "        self.n_data = len(input_data)\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.var_data = var_data\n",
    "        \n",
    "        if self.n_inputs == 0:\n",
    "            p_ave = np.zeros(3)\n",
    "            n_count = self.n_data\n",
    "            for i in range(0,3):\n",
    "                p_ave[i] = np.sum(var_data == i-1)/n_count\n",
    "        \n",
    "        elif self.n_inputs == 1:\n",
    "            n_count = np.zeros(3)\n",
    "            p_ave = np.zeros((3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                n_count[i] = np.sum(input_data == i-1)\n",
    "                for j in range(0,3):\n",
    "                    p_ave[j,i] = np.sum((input_data[:,0] == i-1)*(var_data == j-1))/n_count[i]\n",
    "            \n",
    "            \n",
    "        elif self.n_inputs == 2:\n",
    "            n_count = np.zeros((3,3))\n",
    "            p_ave = np.zeros((3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    n_count[i,j] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1))\n",
    "                    for k in range(0,3):\n",
    "                        p_ave[k,i,j] = np.sum(\n",
    "                            (input_data[:,0] == i-1)*(input_data[:,1] == j-1)*(var_data == k-1))/n_count[i,j]\n",
    "                        \n",
    "        elif self.n_inputs == 3:\n",
    "            n_count = np.zeros((3,3,3))\n",
    "            p_ave = np.zeros((3,3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    for k in range(0,3):\n",
    "                        n_count[i,j,k] = np.sum(\n",
    "                            (input_data[:,0] == i-1)*(input_data[:,1] == j-1)*(input_data[:,2] == k-1))\n",
    "                        for m in range(0,3):\n",
    "                            p_ave[m,i,j,k] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                *(input_data[:,2] == k-1)*(var_data == m-1))/n_count[i,j,k]\n",
    "                            \n",
    "        elif self.n_inputs == 4:\n",
    "            n_count = np.zeros((3,3,3,3))\n",
    "            p_ave = np.zeros((3,3,3,3,3))\n",
    "            \n",
    "            for i in range(0,3):\n",
    "                for j in range(0,3):\n",
    "                    for k in range(0,3):\n",
    "                        for m in range(0,3):\n",
    "                            n_count[i,j,k,m] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                *(input_data[:,2] == k-1)*(input_data[:,3] == m-1))\n",
    "                            for q in range(0,3):\n",
    "                                p_ave[q,i,j,k,m] = np.sum((input_data[:,0] == i-1)*(input_data[:,1] == j-1)\n",
    "                                    *(input_data[:,2] == k-1)*(input_data[:,3] == m-1)\n",
    "                                    *(var_data == q-1))/n_count[i,j,k,m]\n",
    "                        \n",
    "            \n",
    "        else:\n",
    "            print('error -- too many inputs')\n",
    "            return\n",
    "            \n",
    "        self.n_count = torch.tensor(n_count/self.n_data)\n",
    "        self.prob_dist = torch.tensor(p_ave)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def sample(self,data_in=[]):\n",
    "        \n",
    "        if self.n_inputs == 0:\n",
    "            p_temp = self.prob_dist\n",
    "        elif self.n_inputs == 1:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0]+1])\n",
    "        elif self.n_inputs == 2:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0]+1,data_in[1]+1])\n",
    "        elif self.n_inputs == 3:\n",
    "            p_temp = torch.squeeze(self.prob_dist[:,data_in[0]+1,data_in[1]+1,data_in[2]+1])\n",
    "            \n",
    "        else:\n",
    "            print('error -- too many inputs')\n",
    "            p_temp = []\n",
    "        \n",
    "        return torch.squeeze(pyro.sample(self.name,pyro.distributions.Categorical(probs = p_temp)).int()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cg_graph():\n",
    "    \n",
    "    def __init__(self,str_list=[],bel_graph=[], text = []):\n",
    "        \n",
    "        edge_list = []\n",
    "\n",
    "        entity_list = []\n",
    "        # add a new input data type to handle the data from CBN\n",
    "        if text:\n",
    "            \n",
    "            for item in text:\n",
    "                \n",
    "                rel_temp = item.split('*')[1]\n",
    "                \n",
    "                sub_temp = item.split('*')[0]\n",
    "                obj_temp = item.split('*')[2]\n",
    "                \n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                if rel_temp.find('crease') > 0:\n",
    "                    edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "        \n",
    "        if str_list:\n",
    "\n",
    "            for item in str_list:\n",
    "\n",
    "                sub_ind = item.find('=')\n",
    "\n",
    "                sub_temp = item[:sub_ind-1]\n",
    "                obj_temp = item[sub_ind+3:]\n",
    "                \n",
    "                rel_temp = item[sub_ind:sub_ind+2]\n",
    "\n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                # ignore hasVariant, partOf relations\n",
    "                \n",
    "                if rel_temp.find('crease') > 0:\n",
    "                    edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "                # check for duplicate edges\n",
    "                #nodes_temp = [sub_temp,obj_temp]\n",
    "                #list_temp = [[item[0],item[1]] for item in edge_list]\n",
    "                #if nodes_temp in list_temp:\n",
    "                    #ind_temp = list_temp.index(nodes_temp)\n",
    "                    #edge_list[ind_temp][2] += ',' + rel_temp\n",
    "                #else:\n",
    "                    #edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "        elif bel_graph:\n",
    "            \n",
    "            for item in bel_graph.edges:\n",
    "                edge_temp = bel_graph.get_edge_data(item[0],item[1],item[2])\n",
    "                sub_temp = str(item[0]).replace('\"','')\n",
    "                obj_temp = str(item[1]).replace('\"','')\n",
    "                rel_temp = edge_temp['relation']\n",
    "                \n",
    "                if sub_temp not in entity_list:\n",
    "                    entity_list.append(sub_temp)\n",
    "                if obj_temp not in entity_list:\n",
    "                    entity_list.append(obj_temp)\n",
    "                \n",
    "                # ignore hasVariant, partOf relations\n",
    "                \n",
    "                if rel_temp.find('crease') > 0:\n",
    "                    edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "                \n",
    "                # check for duplicate edges\n",
    "                #nodes_temp = [sub_temp,obj_temp]\n",
    "                #list_temp = [[item[0],item[1]] for item in edge_list]\n",
    "                #if nodes_temp in list_temp:\n",
    "                    #ind_temp = list_temp.index(nodes_temp)\n",
    "                    #edge_list[ind_temp][2] += ',' + rel_temp\n",
    "                #else:\n",
    "                    #edge_list.append([sub_temp,obj_temp,rel_temp])\n",
    "\n",
    "        n_nodes = len(entity_list)\n",
    "        self.n_nodes = n_nodes\n",
    "\n",
    "        adj_mat = np.zeros((n_nodes,n_nodes),dtype=int)\n",
    "\n",
    "        for item in edge_list:\n",
    "            out_ind = entity_list.index(item[0])\n",
    "            in_ind = entity_list.index(item[1])\n",
    "            adj_mat[out_ind,in_ind] = 1\n",
    "            \n",
    "        self.edge_list = edge_list\n",
    "        self.entity_list = entity_list\n",
    "        \n",
    "        #self.graph = nx.DiGraph(adj_mat)\n",
    "        \n",
    "        node_dict = {}\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            node_dict[entity_list[i]] = cg_node(np.sum(adj_mat[:,i]),entity_list[i])\n",
    "        \n",
    "        self.node_dict = node_dict\n",
    "        \n",
    "        #self.parent_ind_list = []\n",
    "        #self.child_ind_list = []\n",
    "        self.parent_name_dict = {}\n",
    "        #self.child_name_dict = {}\n",
    "        \n",
    "        self.parent_ind_list = [np.where(adj_mat[:,i] > 0)[0] for i in range(0,n_nodes)]\n",
    "        #self.child_ind_list = [np.where(self.adj_mat[i,:] > 0)[0] for i in range(0,n_nodes)]\n",
    "        \n",
    "        for i in range(0,n_nodes):\n",
    "            self.parent_name_dict[entity_list[i]] = [entity_list[item] for item in self.parent_ind_list[i]]\n",
    "            #self.child_name_dict[entity_list[i]] = [entity_list[item] for item in self.child_ind_list[i]]\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def prob_init(self,data_in):\n",
    "        # initialize all of the nodes\n",
    "        \n",
    "        exog_list = []\n",
    "        prob_dict = {}\n",
    "        \n",
    "        for name in self.node_dict:\n",
    "            i = self.entity_list.index(name)\n",
    "            data_in_temp = data_in[:,self.parent_ind_list[i]]\n",
    "            data_out_temp = data_in[:,i]\n",
    "            \n",
    "            self.node_dict[name].p_init(data_in_temp,data_out_temp)\n",
    "            \n",
    "            if self.node_dict[name].n_inputs == 0:\n",
    "                exog_list.append(name)\n",
    "            prob_dict[name] = self.node_dict[name].prob_dist\n",
    "        \n",
    "        self.exog_list = exog_list\n",
    "        self.prob_dict = prob_dict\n",
    "\n",
    "        return\n",
    "        \n",
    "    def model_sample(self):\n",
    "        \n",
    "        # define exogenous samples\n",
    "        \n",
    "        sample_dict = {}\n",
    "        \n",
    "        for item in self.exog_list:\n",
    "            sample_dict[item] = self.node_dict[item].sample()\n",
    "            \n",
    "        flag = 0\n",
    "        while flag == 0:\n",
    "            \n",
    "            # find all nodes not in sample_dict with parents entirely in sample dict and sample those nodes\n",
    "            for item in self.entity_list:\n",
    "                if (item not in sample_dict \n",
    "                    and np.all([item2 in sample_dict for item2 in self.parent_name_dict[item]])):\n",
    "                    \n",
    "                    sample_dict[item] = self.node_dict[item].sample(\n",
    "                        [sample_dict[item2] for item2 in self.parent_name_dict[item]])\n",
    "            \n",
    "            # if sample dict has all of the nodes in entity list, stop\n",
    "            if sorted([item for item in sample_dict]) == sorted(self.entity_list):\n",
    "                flag = 1\n",
    "            \n",
    "        \n",
    "        return sample_dict\n",
    "    \n",
    "    def model_cond_sample(self,data_dict):\n",
    "        \n",
    "        data_in = {}\n",
    "        for item in data_dict:\n",
    "            data_in[item] = data_dict[item] + 1\n",
    "        \n",
    "        cond_model = pyro.condition(self.model_sample,data=data_in)\n",
    "        return cond_model()\n",
    "        \n",
    "    def model_do_sample(self,do_dict):\n",
    "        \n",
    "        data_in = {}\n",
    "        for item in do_dict:\n",
    "            data_in[item] = do_dict[item] + 1\n",
    "        \n",
    "        do_model = pyro.do(self.model_sample,data=data_in)\n",
    "        return do_model()\n",
    "    \n",
    "    def model_do_cond_sample(self,do_dict,data_dict):\n",
    "        \n",
    "        if np.any([[item1 == item2 for item1 in do_dict] for item2 in data_dict]):\n",
    "            print('overlapping lists!')\n",
    "            return\n",
    "        else:\n",
    "            do_dict_in = {}\n",
    "            for item in do_dict:\n",
    "                do_dict_in[item] = do_dict[item] + 1\n",
    "                \n",
    "            data_dict_in = {}\n",
    "            for item in data_dict:\n",
    "                data_dict_in[item] = data_dict[item] + 1\n",
    "            \n",
    "            do_model = pyro.do(self.model_sample,data=do_dict_in)\n",
    "            cond_model = pyro.condition(do_model,data=data_dict_in)\n",
    "            return cond_model()\n",
    "    \n",
    "    def model_counterfact(self,obs_dict,do_dict_counter):\n",
    "        # find conditional distribution on exogenous variables given observations and do variable values\n",
    "        cond_dict = self.model_cond_sample(obs_dict)\n",
    "        cond_dict_temp = {}\n",
    "        for item in self.exog_list:\n",
    "            cond_dict_temp[item] = cond_dict[item]\n",
    "        \n",
    "        # evaluate observed variables given this condition distribution and do_dict_counter do-variables\n",
    "        return self.model_do_cond_sample(do_dict_counter,cond_dict_temp)\n",
    "        \n",
    "        \n",
    "    def cond_mut_info(self,target,test,cond,data_in):\n",
    "        \n",
    "        cond_temp = cond\n",
    "        \n",
    "        if not cond:\n",
    "            # find parents of target\n",
    "            for item in target:\n",
    "                for item2 in self.parent_name_dict[item]:\n",
    "                    if item2 not in cond_temp:\n",
    "                        cond_temp.append(item2)\n",
    "        \n",
    "        \n",
    "        target_inds = [self.entity_list.index(item) for item in target]\n",
    "        test_inds = [self.entity_list.index(item) for item in test]\n",
    "        cond_inds = [self.entity_list.index(item) for item in cond_temp]\n",
    "        \n",
    "        n_total = len(data_in)\n",
    "        \n",
    "        null_joint = data_in[:,target_inds + cond_inds]\n",
    "        null_cond = data_in[:,cond_inds]\n",
    "        \n",
    "        hypth_joint = data_in[:,target_inds + test_inds + cond_inds]\n",
    "        hypth_cond = data_in[:,test_inds + cond_inds]\n",
    "        \n",
    "        null_entropy = 0\n",
    "        null_list = []\n",
    "        \n",
    "        hypth_entropy = 0\n",
    "        hypth_list = []\n",
    "        for i in range(0,n_total):\n",
    "\n",
    "            if np.all([np.any(null_joint[i,:] != item) for item in null_list]):\n",
    "                num_sum = np.sum([np.all(null_joint[i,:] == item) for item in null_joint])\n",
    "                denom_sum = np.sum([np.all(null_cond[i,:] == item) for item in null_cond])\n",
    "                null_entropy = null_entropy - num_sum*np.log(num_sum/denom_sum)\n",
    "                null_list.append(null_joint[i,:])\n",
    "            \n",
    "            if np.all([np.any(hypth_joint[i,:] != item) for item in hypth_list]):\n",
    "                num_sum = np.sum([np.all(hypth_joint[i,:] == item) for item in hypth_joint])\n",
    "                denom_sum = np.sum([np.all(hypth_cond[i,:] == item) for item in hypth_cond])\n",
    "                hypth_entropy = hypth_entropy - num_sum*np.log(num_sum/denom_sum)\n",
    "                hypth_list.append(hypth_joint[i,:])\n",
    "                \n",
    "        return (null_entropy - hypth_entropy)/n_total\n",
    "        \n",
    "    def g_test(self,name,data_in):\n",
    "        # do the G-test on a single variable of interest\n",
    "        \n",
    "        #p_name = self.calc_prob(name)*len(data_in)\n",
    "        # generate an empirical distribution for variable name\n",
    "        model_data = np.zeros(len(data_in))\n",
    "        for i in range(0,len(data_in)):\n",
    "            model_data[i] = self.model_sample()[name[0]].item()\n",
    "            \n",
    "        p_model = torch.Tensor([np.sum(model_data == -1),np.sum(model_data == 0),np.sum(model_data == 1)])\n",
    "        print(p_model)\n",
    "        \n",
    "        name_ind = self.entity_list.index(name[0])\n",
    "        name_data = data_in[:,name_ind]\n",
    "        \n",
    "        p_data = torch.Tensor([np.sum(name_data == -1),np.sum(name_data == 0),np.sum(name_data == 1)])\n",
    "        print(p_data)\n",
    "        \n",
    "        g_val = 2*torch.sum(p_data*torch.log(p_data/p_model))\n",
    "        \n",
    "        dof = len(data_in) - 1\n",
    "        \n",
    "        p_val = 1-sp.stats.chi2.cdf(g_val.item(), dof)\n",
    "        \n",
    "        return g_val,p_val\n",
    "        \n",
    "  \n",
    "    def write_to_cf(self,filename,spacing):\n",
    "        # write the causal graph to a text file to import into causal fusion\n",
    "        \n",
    "        pos_dict = nx.drawing.layout.planar_layout(self.graph)\n",
    "        \n",
    "        write_dict = {}\n",
    "        write_dict['name'] = 'causal_graph'\n",
    "        \n",
    "        # write nodes\n",
    "        write_dict['nodes'] = []\n",
    "        for i in range(0,len(self.entity_list)):\n",
    "            name = self.entity_list[i]\n",
    "            \n",
    "            write_dict['nodes'].append({})\n",
    "            \n",
    "            write_dict['nodes'][-1]['id'] = 'node' + str(i)\n",
    "            write_dict['nodes'][-1]['name'] = name\n",
    "            write_dict['nodes'][-1]['label'] = name\n",
    "            write_dict['nodes'][-1]['type'] = 'basic'\n",
    "            write_dict['nodes'][-1]['metadata'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['x'] = spacing*pos_dict[i][0]\n",
    "            write_dict['nodes'][-1]['metadata']['y'] = spacing*pos_dict[i][1]\n",
    "            write_dict['nodes'][-1]['metadata']['label'] = ''\n",
    "            write_dict['nodes'][-1]['metadata']['shape'] = 'ellipse'\n",
    "            write_dict['nodes'][-1]['metadata']['fontSize'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['sizeLabelMode'] = 5\n",
    "            write_dict['nodes'][-1]['metadata']['font'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['font']['size'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['size'] = 14\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeId'] = 'node' + str(i) + 'ID'\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelNodeOffset']['y'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['labelOffset']['y'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow'] = {}\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['color'] = '#00000080'\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['size'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['x'] = 0\n",
    "            write_dict['nodes'][-1]['metadata']['shadow']['y'] = 0\n",
    "            \n",
    "        # write edges\n",
    "        write_dict['edges'] = []\n",
    "        \n",
    "        for i in range(0,len(self.edge_list)):\n",
    "            \n",
    "            item = self.edge_list[i]\n",
    "            from_node = self.entity_list.index(item[0])\n",
    "            to_node = self.entity_list.index(item[1])\n",
    "            \n",
    "            write_dict['edges'].append({})\n",
    "            \n",
    "            write_dict['edges'][-1]['id'] = 'node' + str(from_node) + '->node' + str(to_node)\n",
    "            write_dict['edges'][-1]['from'] = item[0]\n",
    "            write_dict['edges'][-1]['to'] = item[1]\n",
    "            write_dict['edges'][-1]['type'] = 'directed'\n",
    "            write_dict['edges'][-1]['metadata'] = {}\n",
    "            write_dict['edges'][-1]['metadata']['isLabelDraggable'] = True\n",
    "            write_dict['edges'][-1]['metadata']['label'] = ''\n",
    "            \n",
    "        \n",
    "        write_dict['task'] = {}\n",
    "        \n",
    "        write_dict['metadata'] = {}\n",
    "        \n",
    "        write_dict['project_id'] = '123456789'\n",
    "        write_dict['_fileType'] = 'graph'\n",
    "                \n",
    "        with open(filename + '.json', 'w') as json_file:\n",
    "            json.dump(write_dict, json_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Epithelial Innate Immune Activation-2.0-Hs.jgf')\n",
    "Epth = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('COVID19.jgf')\n",
    "covid19 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Epth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the type directly from JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CBN website categorize some other relation as causal effect which are not included in this notebook: association(3), positiveCorrolation(4), negativeCorrolation(1), biomarkerFor(2). Thus, we only have 215 causal edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type has the structur of (type_parent, type_children, type_relation):[[edges]]\n",
    "\n",
    "types = {}\n",
    "nodes = set()\n",
    "parents = set()\n",
    "children = set()\n",
    "str_list = []\n",
    "for edges in covid19['graph']['edges']:\n",
    "    parent = edges['source']\n",
    "    child = edges['target']\n",
    "    relation = edges['relation']\n",
    "    text = edges['label']\n",
    "    if relation.find('crease') > 0:\n",
    "        nodes.add(parent)\n",
    "        nodes.add(child)\n",
    "        parents.add(parent)\n",
    "        children.add(child)\n",
    "        str_list.append(parent + '*' + relation + '*' + child)   \n",
    "        parent_type = parent[:parent.find('(')]\n",
    "        chidren_type = child[:child.find('(')]\n",
    "        types.setdefault((parent_type,chidren_type,relation), []).append(parent + '*' + relation + '*' + child) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "99\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "print(len(nodes))\n",
    "print(len(str_list))\n",
    "print(len(types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 140 nodes, 54 edges and 60 type of causal edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use bel2pyro model to extract the types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We still need to extract the string list first from the JSON format data to get the correct pyro model\n",
    "graph_test = cg_graph(text = str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "print(len(graph_test.node_dict))\n",
    "print(len(graph_test.edge_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {}\n",
    "for edge in graph_test.edge_list:\n",
    "    parent_type = edge[0][:edge[0].find('(')]\n",
    "    chidren_type = edge[1][:edge[1].find('(')]\n",
    "    types.setdefault((parent_type,chidren_type,edge[2]), []).append(edge) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the same result when using the bey2pyro model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covid19KG v0.0.1-dev\n",
      "Number of Nodes: 3954\n",
      "Number of Edges: 9484\n",
      "Number of Citations: 185\n",
      "Number of Authors: 950\n",
      "Network Density: 6.07E-04\n",
      "Number of Components: 29\n",
      "Number of Warnings: 0\n"
     ]
    }
   ],
   "source": [
    "# new data covid knowledge graph\n",
    "graph = covid19kg.get_graph()\n",
    "graph.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_test = cg_graph(bel_graph = graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3954\n",
      "2324\n"
     ]
    }
   ],
   "source": [
    "print(len(graph_test.node_dict))\n",
    "print(len(graph_test.edge_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pybel.struct.graph.BELGraph"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## refining of type and label of the BEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type has the structure of (type_parent, type_children, type_relation):[[edges]]\n",
    "label_dict = {'Abundance':['a', 'r', 'm', 'g', 'p','pop', 'composite', 'complex','frag','fus','loc','pmod','var'],\n",
    "             'Process': ['bp', 'path','act'],\n",
    "             'Transformation':['sec','surf','deg','rxn','tloc','fromLoc','products','reactants','toLoc']}\n",
    "def get_information(jgf_file):\n",
    "    # causal_relations = ['increases', 'decreases', 'directlyIncreases', 'directlyDecreases']\n",
    "    types = []\n",
    "    # nodes = set()\n",
    "    parents = []\n",
    "    parent_types = []\n",
    "    children_types = []\n",
    "    parent_labels = []\n",
    "    children_labels = []\n",
    "    children = []\n",
    "    str_list = []\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    for edges in jgf_file['graph']['edges']:\n",
    "        parent = edges['source']\n",
    "        child = edges['target']\n",
    "        relation = edges['relation']\n",
    "        text = edges['label']\n",
    "        if relation.find('crease') > 0:\n",
    "            # nodes.add(parent)\n",
    "            # nodes.add(child)\n",
    "            parents.append(parent)\n",
    "            children.append(child)\n",
    "            str_list.append(parent + '*' + relation + '*' + child)   \n",
    "            parent_type = parent[:parent.find('(')]\n",
    "            children_type = child[:child.find('(')]\n",
    "            types.append(relation)\n",
    "            children_types.append(children_type)\n",
    "            parent_types.append(parent_type)\n",
    "            for label in label_dict:\n",
    "                if parent_type in label_dict[label]:\n",
    "                    parent_labels.append(label)\n",
    "                elif parent_type == '': \n",
    "                    parent_labels.append('Others')\n",
    "            for label in label_dict:\n",
    "                if children_type in label_dict[label]:     \n",
    "                    children_labels.append(label)\n",
    "                elif children_type == '': \n",
    "                    children_labels.append('Others')\n",
    "                    break\n",
    "            # types.setdefault((parent_type,chidren_type,relation), []).append(parent + '*' + relation + '*' + child)\n",
    "    df['parents'] = parents\n",
    "    df['children'] = children\n",
    "    df['types'] = types\n",
    "    df['statements'] = str_list\n",
    "    df['parent_types'] = parent_types\n",
    "    df['parent_labels'] = parent_labels\n",
    "    df['children_types'] = children_types\n",
    "    df['children_labels'] = children_labels\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_information(covid19)\n",
    "df = df[~df['children_labels'].isin(['Others'])] # filter the two with nested structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>parents</th>\n",
       "      <th>children</th>\n",
       "      <th>statements</th>\n",
       "      <th>parent_types</th>\n",
       "      <th>children_types</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <th>parent_labels</th>\n",
       "      <th>children_labels</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">decreases</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">Abundance</th>\n",
       "      <th>Abundance</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Process</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformation</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Process</th>\n",
       "      <th>Abundance</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directlyDecreases</th>\n",
       "      <th>Abundance</th>\n",
       "      <th>Process</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directlyIncreases</th>\n",
       "      <th>Abundance</th>\n",
       "      <th>Process</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">increases</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">Abundance</th>\n",
       "      <th>Abundance</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Process</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformation</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Process</th>\n",
       "      <th>Abundance</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Process</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 parents  children  \\\n",
       "types             parent_labels children_labels                      \n",
       "decreases         Abundance     Abundance             22        22   \n",
       "                                Process                5         5   \n",
       "                                Transformation         1         1   \n",
       "                  Process       Abundance              3         3   \n",
       "directlyDecreases Abundance     Process                2         2   \n",
       "directlyIncreases Abundance     Process                1         1   \n",
       "increases         Abundance     Abundance             21        21   \n",
       "                                Process               15        15   \n",
       "                                Transformation         2         2   \n",
       "                  Process       Abundance             22        22   \n",
       "                                Process                3         3   \n",
       "\n",
       "                                                 statements  parent_types  \\\n",
       "types             parent_labels children_labels                             \n",
       "decreases         Abundance     Abundance                22            22   \n",
       "                                Process                   5             5   \n",
       "                                Transformation            1             1   \n",
       "                  Process       Abundance                 3             3   \n",
       "directlyDecreases Abundance     Process                   2             2   \n",
       "directlyIncreases Abundance     Process                   1             1   \n",
       "increases         Abundance     Abundance                21            21   \n",
       "                                Process                  15            15   \n",
       "                                Transformation            2             2   \n",
       "                  Process       Abundance                22            22   \n",
       "                                Process                   3             3   \n",
       "\n",
       "                                                 children_types  \n",
       "types             parent_labels children_labels                  \n",
       "decreases         Abundance     Abundance                    22  \n",
       "                                Process                       5  \n",
       "                                Transformation                1  \n",
       "                  Process       Abundance                     3  \n",
       "directlyDecreases Abundance     Process                       2  \n",
       "directlyIncreases Abundance     Process                       1  \n",
       "increases         Abundance     Abundance                    21  \n",
       "                                Process                      15  \n",
       "                                Transformation                2  \n",
       "                  Process       Abundance                    22  \n",
       "                                Process                       3  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats = df.groupby(['types','parent_labels','children_labels']).count()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(node_type):\n",
    "    ## we have one category in children nodes pop which we have no information about\n",
    "    for label in label_dict:\n",
    "        if label == 'Abundance':\n",
    "            \n",
    "    if node_type in ['a', 'r', 'm', 'g', 'p', 'composite', 'complex']:\n",
    "        ## this is an abundance type node\n",
    "        ## chose lognormal because we need a +ve continuous distribution here\n",
    "        return dist.LogNormal(torch.tensor([0.0]), torch.tensor([1.0])\n",
    "    if node_type in ['p', 'bp', 'path']: \n",
    "      ## processes have binary distribution. path is a pathology process\n",
    "        return dist.Categorical(torch.tensor([0.5]), torch.tensor([0.5]))\n",
    "    if node_type in ['act', 'molecularActivity', 'chap', 'pep', 'ribo']:\n",
    "      ## activity is continuous                       \n",
    "        return dist.LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    if node_type in ['reaction', 'degradation']:\n",
    "      ## it should be continuous so starting with lognormal\n",
    "        return dist.LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    if node_type in ['tloc', 'sec', 'surf', 'tscript', 'tport']:\n",
    "      ## transport category\n",
    "        return dist.LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "    if node_type in['gtp', 'cat', 'kin', 'phos']:\n",
    "      ## these are binary\n",
    "        return dist.Categorical(torch.tensor([0.5]), torch.tensor([0.5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
